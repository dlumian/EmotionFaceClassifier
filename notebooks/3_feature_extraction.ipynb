{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f3a450-d3f4-4e18-9a97-93f4b4d892be",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "## Emotion Face Classifier Notebook 3\n",
    "\n",
    "Uses unsupervised decomposition models to extract averaged features of images by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5ba44f-8d05-4bc3-bc37-8342bf24f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7004192-0562-4786-898e-a5a6ee97776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef616a3f-417d-4c75-b402-70855ccee1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe936733-29e0-4b2a-918b-ecc2aaed23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d2509a7-8d5d-4e83-beec-8f865dc66399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascifuncs.tidbit_tools import load_json, write_json, print_json, check_directory_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c2ce467-6979-4a59-bc52-da13108c04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory set to /Users/dsl/Documents/GitHub/EmotionFaceClassifier, matches target dir string EmotionFaceClassifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dir = 'EmotionFaceClassifier'\n",
    "check_directory_name(main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac803d0-d11d-4b75-86f5-8080c99f34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.feature_extraction import perform_unsupervised_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7e2a7b-f031-4568-96f9-88ee82db26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dicts = load_json('./configs/input_mappings.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9514b553-0de4-43c1-928f-0e74d12e0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_colors = common_dicts['plotly_styles']['Training']['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b85847b-b60a-4250-92fa-9447b7b71d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in FER 2013 data\n",
    "fer2013_path = 'data/fer2013_paths.csv'\n",
    "fer2013 = pd.read_csv(fer2013_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a9cd4ff-8e2c-4bf2-9d23-66848d216000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_id</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "      <th>emotion</th>\n",
       "      <th>image</th>\n",
       "      <th>usage</th>\n",
       "      <th>emo_count_id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "      <td>Angry</td>\n",
       "      <td>[[ 70  80  82 ...  52  43  41]\\n [ 65  61  58 ...</td>\n",
       "      <td>Training</td>\n",
       "      <td>1</td>\n",
       "      <td>data/Training/Angry/Angry-1.jpg</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "      <td>Angry</td>\n",
       "      <td>[[151 150 147 ... 129 140 120]\\n [151 149 149 ...</td>\n",
       "      <td>Training</td>\n",
       "      <td>2</td>\n",
       "      <td>data/Training/Angry/Angry-2.jpg</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "      <td>Fear</td>\n",
       "      <td>[[231 212 156 ...  44  27  16]\\n [229 175 148 ...</td>\n",
       "      <td>Training</td>\n",
       "      <td>1</td>\n",
       "      <td>data/Training/Fear/Fear-1.jpg</td>\n",
       "      <td>slategray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "      <td>Sad</td>\n",
       "      <td>[[ 24  32  36 ... 173 172 173]\\n [ 25  34  29 ...</td>\n",
       "      <td>Training</td>\n",
       "      <td>1</td>\n",
       "      <td>data/Training/Sad/Sad-1.jpg</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[[ 4  0  0 ... 27 24 25]\\n [ 1  0  0 ... 26 23...</td>\n",
       "      <td>Training</td>\n",
       "      <td>1</td>\n",
       "      <td>data/Training/Neutral/Neutral-1.jpg</td>\n",
       "      <td>sienna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion_id                                             pixels     Usage  \\\n",
       "0           0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training   \n",
       "1           0  151 150 147 155 148 133 111 140 170 174 182 15...  Training   \n",
       "2           2  231 212 156 164 174 138 161 173 182 200 106 38...  Training   \n",
       "3           4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training   \n",
       "4           6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training   \n",
       "\n",
       "   emotion                                              image     usage  \\\n",
       "0    Angry  [[ 70  80  82 ...  52  43  41]\\n [ 65  61  58 ...  Training   \n",
       "1    Angry  [[151 150 147 ... 129 140 120]\\n [151 149 149 ...  Training   \n",
       "2     Fear  [[231 212 156 ...  44  27  16]\\n [229 175 148 ...  Training   \n",
       "3      Sad  [[ 24  32  36 ... 173 172 173]\\n [ 25  34  29 ...  Training   \n",
       "4  Neutral  [[ 4  0  0 ... 27 24 25]\\n [ 1  0  0 ... 26 23...  Training   \n",
       "\n",
       "   emo_count_id                             img_path      color  \n",
       "0             1      data/Training/Angry/Angry-1.jpg        red  \n",
       "1             2      data/Training/Angry/Angry-2.jpg        red  \n",
       "2             1        data/Training/Fear/Fear-1.jpg  slategray  \n",
       "3             1          data/Training/Sad/Sad-1.jpg       blue  \n",
       "4             1  data/Training/Neutral/Neutral-1.jpg     sienna  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fer2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48450ff3-1948-4152-8415-c8bb0efb6b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 9)\n",
      "(28709, 9)\n"
     ]
    }
   ],
   "source": [
    "# Select training data\n",
    "print(fer2013.shape)\n",
    "train_df = fer2013[fer2013['usage']=='Training']\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0add6d4-7feb-4804-8bc5-609967f66f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, X, y = run_analysis(train_df, 'img_path', 'emotion', 'configs/unsupervised_models.json', 'PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a716670-cb74-4d2b-9199-e18d77eb1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.feature_extraction import create_enhanced_matrix_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e92091ae-12db-46fa-8ec4-4dad0e38db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create plots for each normalization method\n",
    "# for normalizer, result in results.items():\n",
    "#     fig = create_enhanced_matrix_plot(result, 'PCA', emotion_colors, normalizer)\n",
    "#     plt.savefig(f'PCA_reconstruction_matrix_{normalizer}.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f828fc-fa5c-477c-8d8d-2ca6e74a0992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0155aa6a-cf84-43fc-8ce6-e047684d48fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (28709, 2304), y shape: (28709,)\n",
      "X_flattened shape: (28709, 2304)\n",
      "Normalized X shape: (28709, 2304)\n",
      "Extracted features shape: (28709, 100)\n",
      "Model n_components: 100\n",
      "Model components shape: (100, 2304)\n",
      "Input features shape: (28709, 100)\n",
      "Partial features shape for 1 components: (28709, 100)\n",
      "Reconstruction shape for 1 components: (28709, 2304)\n",
      "Partial features shape for 10 components: (28709, 100)\n",
      "Reconstruction shape for 10 components: (28709, 2304)\n",
      "Partial features shape for 25 components: (28709, 100)\n",
      "Reconstruction shape for 25 components: (28709, 2304)\n",
      "Partial features shape for 50 components: (28709, 100)\n",
      "Reconstruction shape for 50 components: (28709, 2304)\n",
      "Partial features shape for 75 components: (28709, 100)\n",
      "Reconstruction shape for 75 components: (28709, 2304)\n",
      "Partial features shape for 100 components: (28709, 100)\n",
      "Reconstruction shape for 100 components: (28709, 2304)\n",
      "Number of unique categories: 7\n",
      "Unique categories: Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise\n",
      "Normalized X shape: (28709, 2304)\n",
      "Extracted features shape: (28709, 100)\n",
      "Model n_components: 100\n",
      "Model components shape: (100, 2304)\n",
      "Input features shape: (28709, 100)\n",
      "Partial features shape for 1 components: (28709, 100)\n",
      "Reconstruction shape for 1 components: (28709, 2304)\n",
      "Partial features shape for 10 components: (28709, 100)\n",
      "Reconstruction shape for 10 components: (28709, 2304)\n",
      "Partial features shape for 25 components: (28709, 100)\n",
      "Reconstruction shape for 25 components: (28709, 2304)\n",
      "Partial features shape for 50 components: (28709, 100)\n",
      "Reconstruction shape for 50 components: (28709, 2304)\n",
      "Partial features shape for 75 components: (28709, 100)\n",
      "Reconstruction shape for 75 components: (28709, 2304)\n",
      "Partial features shape for 100 components: (28709, 100)\n",
      "Reconstruction shape for 100 components: (28709, 2304)\n",
      "Number of unique categories: 7\n",
      "Unique categories: Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise\n",
      "Normalized X shape: (28709, 2304)\n",
      "Extracted features shape: (28709, 100)\n",
      "Model n_components: 100\n",
      "Model components shape: (100, 2304)\n",
      "Input features shape: (28709, 100)\n",
      "Partial features shape for 1 components: (28709, 100)\n",
      "Reconstruction shape for 1 components: (28709, 2304)\n",
      "Partial features shape for 10 components: (28709, 100)\n",
      "Reconstruction shape for 10 components: (28709, 2304)\n",
      "Partial features shape for 25 components: (28709, 100)\n",
      "Reconstruction shape for 25 components: (28709, 2304)\n",
      "Partial features shape for 50 components: (28709, 100)\n",
      "Reconstruction shape for 50 components: (28709, 2304)\n",
      "Partial features shape for 75 components: (28709, 100)\n",
      "Reconstruction shape for 75 components: (28709, 2304)\n",
      "Partial features shape for 100 components: (28709, 100)\n",
      "Reconstruction shape for 100 components: (28709, 2304)\n",
      "Number of unique categories: 7\n",
      "Unique categories: Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise\n",
      "Results for pca_none_100 already exist. Skipping...\n",
      "Results for pca_minmax_100 already exist. Skipping...\n",
      "Results for pca_standard_100 already exist. Skipping...\n",
      "X shape: (28709, 2304), y shape: (28709,)\n",
      "X_flattened shape: (28709, 2304)\n",
      "Normalized X shape: (28709, 2304)\n",
      "Extracted features shape: (28709, 50)\n",
      "Model n_components: 50\n",
      "Model components shape: (50, 2304)\n",
      "Input features shape: (28709, 50)\n",
      "Partial features shape for 1 components: (28709, 50)\n",
      "Reconstruction shape for 1 components: (28709, 2304)\n",
      "Partial features shape for 5 components: (28709, 50)\n",
      "Reconstruction shape for 5 components: (28709, 2304)\n",
      "Partial features shape for 15 components: (28709, 50)\n",
      "Reconstruction shape for 15 components: (28709, 2304)\n",
      "Partial features shape for 30 components: (28709, 50)\n",
      "Reconstruction shape for 30 components: (28709, 2304)\n",
      "Partial features shape for 50 components: (28709, 50)\n",
      "Reconstruction shape for 50 components: (28709, 2304)\n",
      "Number of unique categories: 7\n",
      "Unique categories: Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise\n",
      "Normalized X shape: (28709, 2304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (28709, 50)\n",
      "Model n_components: 50\n",
      "Model components shape: (50, 2304)\n",
      "Input features shape: (28709, 50)\n",
      "Partial features shape for 1 components: (28709, 50)\n",
      "Reconstruction shape for 1 components: (28709, 2304)\n",
      "Partial features shape for 5 components: (28709, 50)\n",
      "Reconstruction shape for 5 components: (28709, 2304)\n",
      "Partial features shape for 15 components: (28709, 50)\n",
      "Reconstruction shape for 15 components: (28709, 2304)\n",
      "Partial features shape for 30 components: (28709, 50)\n",
      "Reconstruction shape for 30 components: (28709, 2304)\n",
      "Partial features shape for 50 components: (28709, 50)\n",
      "Reconstruction shape for 50 components: (28709, 2304)\n",
      "Number of unique categories: 7\n",
      "Unique categories: Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise\n",
      "Results for nmf_none_50 already exist. Skipping...\n",
      "Results for nmf_minmax_50 already exist. Skipping...\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m perform_unsupervised_analysis(\n\u001b[1;32m      2\u001b[0m     df\u001b[38;5;241m=\u001b[39mtrain_df,\n\u001b[1;32m      3\u001b[0m     img_path_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     label_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     config_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigs/unsupervised_models.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     analysis_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPCA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNMF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mICA\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m     emotion_colors\u001b[38;5;241m=\u001b[39memotion_colors\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/EmotionFaceClassifier/utils/feature_extraction.py:237\u001b[0m, in \u001b[0;36mperform_unsupervised_analysis\u001b[0;34m(df, img_path_column, label_column, config_path, analysis_types, emotion_colors)\u001b[0m\n\u001b[1;32m    234\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(base_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m analysis_type \u001b[38;5;129;01min\u001b[39;00m analysis_types:\n\u001b[0;32m--> 237\u001b[0m     results, X, y \u001b[38;5;241m=\u001b[39m run_analysis(df, img_path_column, label_column, config_path, analysis_type)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m normalizer, result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    240\u001b[0m         total_components \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mn_components_\n",
      "File \u001b[0;32m~/Documents/GitHub/EmotionFaceClassifier/utils/feature_extraction.py:132\u001b[0m, in \u001b[0;36mrun_analysis\u001b[0;34m(df, img_path_column, label_column, config_path, analysis_type, subset_condition, flatten, img_size)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mRun the complete analysis pipeline for a given analysis type.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mdict: Results of the analysis\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m config \u001b[38;5;241m=\u001b[39m load_json(config_path)  \u001b[38;5;66;03m# Assuming you're using your own load_json function\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m analysis_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnalyses\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m analysis_type)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Apply subset condition if provided\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subset_condition:\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "perform_unsupervised_analysis(\n",
    "    df=train_df,\n",
    "    img_path_column='img_path',\n",
    "    label_column='emotion',\n",
    "    config_path='configs/unsupervised_models.json',\n",
    "    analysis_types=['PCA', 'NMF', 'ICA'],\n",
    "    emotion_colors=emotion_colors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0698ec1-b3c2-4109-8053-d9c57ed88ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca845d6-a5b6-45f2-b361-8050ab18edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_results, X_flat, y = run_analysis(train_df, 'img_path', 'emotion', 'configs/unsupervised_models.json', 'PCA', flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528178c8-392d-4256-a574-0f4c5754674c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9b674b-d14c-4958-9774-cd5b8cb80ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa6070-7573-4e43-8393-25a196d9aeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239839cd-130b-4037-9f98-bd3a204f942e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f918d6f-0fca-4674-abcd-4e5f8432c4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb1989-2c38-44ea-bd73-36d02addeeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pca_results['standard']['metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a80d7-9c7c-40f7-afb2-35643a643fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load your data and categories here\n",
    "# data = ...\n",
    "# categories = ...\n",
    "\n",
    "# # Run the analysis for each type\n",
    "# pca_results = run_analysis(data, categories, 'configs/unsupervised_models.json', 'PCA')\n",
    "# nmf_results = run_analysis(data, categories, 'configs/unsupervised_models.json', 'NMF')\n",
    "# ica_results = run_analysis(data, categories, 'configs/unsupervised_models.json', 'FastICA')\n",
    "\n",
    "# # Now you can interactively explore the results\n",
    "# # For example:\n",
    "# print(pca_results['standard']['metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf98e8-6323-44e9-8dfc-82d98a16b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For unsupervised learning (flattened data)\n",
    "# pca_results, X_flat, y = run_analysis(df, 'img_path', 'emotion', 'configs/unsupervised_models.json', 'PCA', flatten=True)\n",
    "\n",
    "# # For supervised or deep learning (matrix data)\n",
    "# # _, X_matrix, y = run_analysis(df, 'img_path', 'emotion', 'configs/unsupervised_models.json', 'PCA', flatten=False, img_size=(48, 48))\n",
    "\n",
    "# # Now you can use X_matrix for your supervised or deep learning models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
