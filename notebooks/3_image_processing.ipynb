{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff98d76-19b6-4f68-96b5-df5c6ac04b0f",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) Image Analysis \n",
    "\n",
    "## Emotion Face Classifier Notebook 3\n",
    "\n",
    "Visuals example images, image properties, and uses unsupervised models for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32a016-6c5b-4966-97da-b65e4b782f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d6bcc-d1e5-47d4-9ab6-5079f6204c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7121b1e-a55e-411b-a994-e2a9d5796d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1e003-42de-4416-8997-91dec6e09e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f22985-b974-4e42-89aa-15d650ee9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascifuncs.tidbit_tools import load_json, write_json, print_json, check_directory_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890eb401-b50a-448f-a82c-b146347ff804",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = 'EmotionFaceClassifier'\n",
    "check_directory_name(main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4803a-89fa-4eb4-9941-e822df6f23f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.image_processing import (\n",
    "    generate_sample_images,\n",
    "    generate_samples_figure,\n",
    "    preprocess_images,\n",
    "    generate_composite_faces,\n",
    "    generate_pixel_intensities,\n",
    "    run_dimensionality_reduction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55d086-7055-473b-a0e2-385840d240f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.analysis_tools import instantiate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea49cf9a-6044-4e07-a9ea-0df70071e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.image_processing import (\n",
    "#     generate_sample_images,\n",
    "#     plot_matrix,\n",
    "#     preprocess_images,\n",
    "#     apply_ticks,\n",
    "#     set_spines_and_titles_by_column,\n",
    "#     add_figure_title,\n",
    "#     add_text_box,\n",
    "#     save_figure\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df949981-42b3-4935-9075-d1006e951b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.image_processing import (\n",
    "#     preprocess_images,\n",
    "#     generate_sample_images,\n",
    "#     plot_face_matrix,\n",
    "#     generate_composite_faces,\n",
    "#     run_dimensionality_reduction,\n",
    "#     generate_pixel_intensities\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c350f91-dd7c-4edf-8986-16d59ac5ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in FER 2013 data\n",
    "fer2013_path = 'data/fer2013_paths.csv'\n",
    "fer2013 = pd.read_csv(fer2013_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a69fa-c0f5-487a-9fda-0c4c1246515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fer2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a7923-748b-4868-bdff-a351b956bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select training data\n",
    "print(fer2013.shape)\n",
    "train_df = fer2013[fer2013['usage']=='Training']\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fea1d3-66c5-4622-804b-5aabc81c9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load common dicts from json config file\n",
    "common_dicts = load_json('./configs/input_mappings.json')\n",
    "# print_json(common_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d8e4b-1e99-441e-b9ee-ee47f4c4159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subset of emo-color mappings\n",
    "color_dict = common_dicts['plotly_styles']['Training']['color']\n",
    "color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d338f-d9f5-4b9a-a326-2e4184016815",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_params = load_json('./configs/plotting_params.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f21385-c5dd-4ce5-b820-545141e35985",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_samples = generate_sample_images(train_df, n=5, cat_col='emotion', path_col='img_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684598aa-2274-42e0-ad2e-da07a94109be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_imgs_save_path = os.path.join('imgs', 'comparisons', 'sample_images.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a62a55-b86d-4b6d-8818-ac5bc4d28d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_samples_figure(\n",
    "        image_dict=emo_samples, \n",
    "        row_labels=None, \n",
    "        plot_params=plot_params, \n",
    "        color_dict=color_dict, \n",
    "        title='Example Faces',\n",
    "        text_box=None,\n",
    "        save_path=sample_imgs_save_path,\n",
    "        dpi=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e1a37-c7d5-4fca-9b25-cd71a85ab956",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocess_images(fer2013, usage='Training', flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64966198-1fc4-496b-9352-6869c1acf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_face_dict, row_labels = generate_composite_faces(X_train, y_train, overall=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a99e75-8c29-43ae-a751-52ee8d860e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_imgs_save_path = os.path.join('imgs', 'comparisons', 'composite_faces.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe0137-6bf8-4538-8235-5695e0e04dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_samples_figure(\n",
    "        image_dict=composite_face_dict, \n",
    "        row_labels=row_labels, \n",
    "        plot_params=plot_params, \n",
    "        color_dict=color_dict, \n",
    "        title='Averaged Composite Faces',\n",
    "        text_box=None,\n",
    "        save_path=composite_imgs_save_path,\n",
    "        dpi=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e5371-d666-49e7-ab22-57cd45402b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_imgs_save_path = os.path.join('imgs', 'comparisons', 'pixel_intensities.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e5a3d-a92c-4836-8d6b-f2fd351c85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_pixel_intensities(X_train, y_train, color_dict=color_dict, save_path=pixel_imgs_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe38e9-d06d-4c5a-8cce-c3ba6799c6ad",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Generates averaged emotional faces by category as reconstructed from a varying number of components using unsupervised decomposition analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04f71e-339d-4d04-aec4-737a40bd3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_models = load_json('./configs/unsupervised_models.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39e265-453a-49d5-ba96-26c1bd173485",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c372175-b472-4a3a-b440-54fb675be3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc52291-78a9-45ea-a158-5aa22c95758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for decomp_model, norm, comps_list in zip(model_keys, norm_options, components_list):\n",
    "#     max_comp = decomp_models[decomp_model]['params']['n_components']\n",
    "#     results, used_components = run_dimensionality_reduction(\n",
    "#         X=X_train, \n",
    "#         y=y_train, \n",
    "#         components_list=comps_list, \n",
    "#         model_dict=decomp_models[decomp_model], \n",
    "#         normalize='none'\n",
    "#     )\n",
    "\n",
    "#     dim_reduce_save_path = os.path.join('imgs', 'comparisons', f'{decomp_model}_{norm}_max_comp_{max_comp}_faces.png')\n",
    "    \n",
    "#     # Generate a matrix plot of the results\n",
    "#     plot_face_matrix(\n",
    "#             results, \n",
    "#             row_labels=used_components, \n",
    "#             group_colors=color_dict,\n",
    "#             save_path=dim_reduce_save_path,\n",
    "#             method=decomp_model,\n",
    "#             norm=norm,\n",
    "#             total_components=max_comp)    \n",
    "#     print(f\"Completed: method={decomp_model}, normalization={norm}, components_list={comps_list}, max_components={max_comp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ffc6e0-2761-4082-9af5-4a0c69ec2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in analyses:\n",
    "#     for norm in a['normalization']:\n",
    "#         print(a['type'], norm)\n",
    "#         type = a['type']\n",
    "#         print(decomp_models[type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b3547-3375-4915-b6a1-3d5ee2d40d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs = decomp_models['Analyses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508519b-1824-46f4-96fb-dfe8eee5d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in model_runs:\n",
    "    print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0a595-2703-47d4-84a7-955cd8e988c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run analysis for a single configuration\n",
    "# for run in model_runs:\n",
    "#     if run['type']=='NMF':\n",
    "#         for norm in run['normalization']:\n",
    "#             try:\n",
    "#                 results, valid_components = run_dimensionality_reduction(\n",
    "#                     X=X_train, \n",
    "#                     y=y_train, \n",
    "#                     model_dict=decomp_models[run['type']],\n",
    "#                     normalization=run[\"normalization\"],\n",
    "#                     total_components=run[\"total_components\"],\n",
    "#                     components_for_reconstruction=run[\"components_for_reconstruction\"]\n",
    "#                 )\n",
    "            \n",
    "#                 # Access valid components used for mapping\n",
    "#                 print(\"Valid components used for reconstruction:\", valid_components)\n",
    "#                 # Access reconstructed images\n",
    "#                 for category, reconstruction in results.items():\n",
    "#                     print(f\"Category: {category}, Reconstruction shape: {reconstruction.shape}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error encountered: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9d395-ce77-43b0-8c56-844e6a37dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2b405-5a5a-457f-a576-8dc4a0912dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da2476-b557-462a-b52f-533de3aba160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12771e38-6929-4479-9d6c-0c55255a1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.decomposition import NMF\n",
    "\n",
    "# # Sample data X\n",
    "# # Replace this with your actual data\n",
    "# # X = np.random.rand(100, 64)  # Example data with 100 samples and 64 features\n",
    "\n",
    "# # Instantiate and fit the NMF model with 50 components\n",
    "# total_components = 50\n",
    "# nmf = NMF(n_components=total_components, init='random', random_state=42)\n",
    "# X_transformed = nmf.fit_transform(X_train)\n",
    "\n",
    "# # Get the full set of components (basis matrix)\n",
    "# components = nmf.components_\n",
    "\n",
    "# # Partial reconstructions using 1, 5, and 10 components\n",
    "# partial_reconstructions = {}\n",
    "# for n_components in [1, 5, 10]:\n",
    "#     # Create partial reconstruction by limiting to the first n_components\n",
    "#     X_partial = X_transformed[:, :n_components] @ components[:n_components, :]\n",
    "#     partial_reconstructions[f'{n_components}_components'] = X_partial\n",
    "\n",
    "# # Output partial reconstructions for inspection\n",
    "# for key, reconstruction in partial_reconstructions.items():\n",
    "#     print(f\"Reconstruction with {key}:\")\n",
    "#     print(reconstruction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c3fe4-a7d9-454e-80f7-375ee01b6eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a8278-8599-4ab6-aca6-94106a572bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f36e73-0701-435c-bc2e-32a9b5cccdc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b3da6-20d2-42b2-813f-7efe2ba19757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5290e2-14de-4dda-90f6-579924e8ecab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f9c80-36c5-462c-a87a-3c4b7565d642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b3266-e18f-48c3-a048-661953a41be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def generate_composite_faces(X, model_dict, n_components, partial_components):\n",
    "#     \"\"\"\n",
    "#     Generates composite faces from a set of flattened images by performing dimensionality reduction\n",
    "#     and reconstructing with partial component values.\n",
    "\n",
    "#     Args:\n",
    "#     - X (np.ndarray): Array of flattened images with shape (n_samples, height * width).\n",
    "#     - model_dict (dict): Dictionary specifying the model type and parameters.\n",
    "#     - n_components (int): Total number of components for the dimensionality reduction.\n",
    "#     - partial_components (list of int): List of component counts to use for partial reconstructions.\n",
    "\n",
    "#     Returns:\n",
    "#     - composite_images (list of np.ndarray): List of composite images, each corresponding\n",
    "#       to a different partial component count, with shape (height, width).\n",
    "#     \"\"\"\n",
    "#     # Get dimensions of the flattened images\n",
    "#     n_samples, flat_size = X.shape\n",
    "#     height = width = int(np.sqrt(flat_size))  # assuming square images\n",
    "\n",
    "#     # Instantiate and fit the dimensionality reduction model\n",
    "#     model_dict['n_components'] = n_components  # Set the total number of components\n",
    "#     model = instantiate_model(model_dict)\n",
    "#     transformed = model.fit_transform(X)\n",
    "\n",
    "#     # List to store composite images for each partial component count\n",
    "#     composite_images = []\n",
    "\n",
    "#     for comp in partial_components:\n",
    "#         # Reconstruct images using only the specified number of components\n",
    "#         truncated_transformed = np.zeros_like(transformed)\n",
    "#         truncated_transformed[:, :comp] = transformed[:, :comp]\n",
    "#         reconstructed = model.inverse_transform(truncated_transformed)\n",
    "\n",
    "#         # Reshape and average the reconstructed images\n",
    "#         reconstructed_images = reconstructed.reshape(n_samples, height, width)\n",
    "#         composite_image = np.mean(reconstructed_images, axis=0)\n",
    "#         composite_images.append(composite_image)\n",
    "\n",
    "#     return composite_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e5e7e-a7d1-40d4-8d9d-c99ae239d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_nmf = decomp_models['NMF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ddc6a-292e-4e2c-9c1a-43695aaf0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_nmf['n_components'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7c562-9b7c-44ff-8621-4ba6d93d7928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980159ea-adaa-4ded-b2ac-fc2d2fba20a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757e5b8-7d03-4ba2-9562-4f5be249dabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f30ad6-d427-49c4-b810-573ead5a8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_results = generate_composite_faces(X=X_train, model_dict=test_nmf, n_components=50, partial_components=[1, 10, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f7f0b-bd0a-4330-8051-c5ab855b3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97214fa2-9d14-4e63-9040-641470f8fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_results[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a67734-e8d7-4a66-8065-089831fb5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def display_composite_images(composite_images, partial_components):\n",
    "#     \"\"\"\n",
    "#     Displays each composite image in a Jupyter notebook.\n",
    "\n",
    "#     Args:\n",
    "#     - composite_images (list of np.ndarray): List of composite images to display.\n",
    "#     - partial_components (list of int): List of component counts used to generate each composite image.\n",
    "#     \"\"\"\n",
    "#     for i, composite_image in enumerate(composite_images):\n",
    "#         plt.figure()\n",
    "#         plt.imshow(composite_image, cmap='gray')\n",
    "#         plt.title(f'Composite Image with {partial_components[i]} Components')\n",
    "#         plt.axis('off')  # Hide axes for a cleaner look\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec524db-edd6-45cc-9664-f1f1fee7016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_composite_images(test_results, [1, 10, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7094a-dd4d-4b3a-a7fc-99805cd9a611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b51b4a-fad2-4bb2-8c94-19a5f5a249ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb34eec-d41e-4e5d-af18-e596b97f624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def generate_composite_faces(X):\n",
    "#     \"\"\"\n",
    "#     Generates composite faces from a set of flattened images by performing PCA and reconstructing \n",
    "#     with partial component values.\n",
    "\n",
    "#     Args:\n",
    "#     - X (np.ndarray): Array of flattened images with shape (n_samples, 48 * 48).\n",
    "\n",
    "#     Returns:\n",
    "#     - composite_images (list of np.ndarray): List of composite images, each corresponding\n",
    "#       to a different partial component count, with shape (48, 48).\n",
    "#     \"\"\"\n",
    "#     # Image dimensions (hardcoded to 48x48 as per your specification)\n",
    "#     height, width = 48, 48\n",
    "\n",
    "#     # Fixed parameters\n",
    "#     n_components = 50\n",
    "#     partial_components = [1, 10, 25, 40]\n",
    "\n",
    "#     # Scale and center the data for PCA\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#     # Apply PCA with n_components set to 50\n",
    "#     pca = PCA(n_components=n_components)\n",
    "#     transformed = pca.fit_transform(X_scaled)\n",
    "\n",
    "#     # List to store composite images for each partial component count\n",
    "#     composite_images = []\n",
    "\n",
    "#     for comp in partial_components:\n",
    "#         # Reconstruct images using only the specified number of components\n",
    "#         truncated_transformed = np.zeros_like(transformed)\n",
    "#         truncated_transformed[:, :comp] = transformed[:, :comp]\n",
    "#         reconstructed = pca.inverse_transform(truncated_transformed)\n",
    "\n",
    "#         # Reverse scaling to original data range\n",
    "#         reconstructed = scaler.inverse_transform(reconstructed)\n",
    "\n",
    "#         # Reshape and average the reconstructed images\n",
    "#         reconstructed_images = reconstructed.reshape(-1, height, width)\n",
    "#         composite_image = np.mean(reconstructed_images, axis=0)\n",
    "#         composite_images.append(composite_image)\n",
    "\n",
    "#     return composite_images\n",
    "\n",
    "# def display_composite_images(composite_images, partial_components):\n",
    "#     \"\"\"\n",
    "#     Displays each composite image in a Jupyter notebook.\n",
    "\n",
    "#     Args:\n",
    "#     - composite_images (list of np.ndarray): List of composite images to display.\n",
    "#     - partial_components (list of int): List of component counts used to generate each composite image.\n",
    "#     \"\"\"\n",
    "#     for i, composite_image in enumerate(composite_images):\n",
    "#         plt.figure()\n",
    "#         plt.imshow(composite_image, cmap='gray')\n",
    "#         plt.title(f'Composite Image with {partial_components[i]} Components')\n",
    "#         plt.axis('off')  # Hide axes for a cleaner look\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c10d2f-5ae9-4c84-9367-f8d3c276171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# composite_images = generate_composite_faces(X_train)\n",
    "# display_composite_images(composite_images, [1, 10, 25, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55143826-8566-47fd-9bde-c850184aa363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def check_allclose_comparisons(composite_images):\n",
    "#     \"\"\"\n",
    "#     Checks whether all composite images in the list are similar using np.allclose.\n",
    "\n",
    "#     Args:\n",
    "#     - composite_images (list of np.ndarray): List of composite images.\n",
    "\n",
    "#     Returns:\n",
    "#     - comparison_results (dict): A dictionary with tuple keys indicating pairs of indices (i, j)\n",
    "#       and boolean values indicating whether the images at those indices are similar.\n",
    "#     \"\"\"\n",
    "#     comparison_results = {}\n",
    "#     num_images = len(composite_images)\n",
    "    \n",
    "#     # Compare each pair of images using np.allclose\n",
    "#     for i in range(num_images):\n",
    "#         for j in range(i + 1, num_images):\n",
    "#             comparison_results[(i, j)] = np.allclose(composite_images[i], composite_images[j])\n",
    "    \n",
    "#     return comparison_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a2abe-967e-486b-85c3-4787789c89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample usage (with hypothetical composite_images list)\n",
    "# results = check_allclose_comparisons(composite_images)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da0574c-8bf1-475a-9264-177cbc46b43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b569e5-c0d7-472d-a819-c3ba79f6b1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ef0a6-b804-456d-a877-888a21c0b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Normalize Data (no normalization for this run)\n",
    "def normalize_data(data, method='none'):\n",
    "    \"\"\"\n",
    "    Applies normalization to the data.\n",
    "    \"\"\"\n",
    "    if method == 'none':\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported normalization method for this test.\")\n",
    "\n",
    "# Step 2: Initialize and Fit NMF Model\n",
    "def fit_nmf_model(X, n_components=100):\n",
    "    \"\"\"\n",
    "    Fits an NMF model with the specified number of components.\n",
    "    \"\"\"\n",
    "    model = NMF(n_components=n_components, init='nndsvda', max_iter=500, random_state=42)\n",
    "    W = model.fit_transform(X)  # Transformed matrix (basis)\n",
    "    H = model.components_       # Coefficients matrix\n",
    "    return model, W, H\n",
    "\n",
    "# Step 3: Generate Partial Reconstructions\n",
    "def generate_partial_reconstructions(model, W, H, reconstruction_components):\n",
    "    \"\"\"\n",
    "    Generates partial reconstructions for a given NMF model and specified components.\n",
    "    \"\"\"\n",
    "    reconstructions = []\n",
    "    for n in reconstruction_components:\n",
    "        # Truncate components to the top `n`\n",
    "        H_partial = H[:n, :]\n",
    "        W_partial = W[:, :n]\n",
    "        # Reconstruct using the truncated components\n",
    "        X_reconstructed = np.dot(W_partial, H_partial)\n",
    "        reconstructions.append(X_reconstructed)\n",
    "    return reconstructions\n",
    "\n",
    "# Main Analysis\n",
    "def main_single_analysis(X):\n",
    "    \"\"\"\n",
    "    Performs a single analysis with NMF (100 components) and specified partial reconstructions.\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    n_components = 100\n",
    "    reconstruction_components = [1, 10, 25, 50, 75, 100]\n",
    "    \n",
    "    # Step 1: Normalize the data\n",
    "    X_normalized = normalize_data(X, method='none')\n",
    "    \n",
    "    # Step 2: Fit NMF model\n",
    "    model, W, H = fit_nmf_model(X_normalized, n_components=n_components)\n",
    "    \n",
    "    # Step 3: Generate partial reconstructions\n",
    "    reconstructions = generate_partial_reconstructions(\n",
    "        model, W, H, reconstruction_components\n",
    "    )\n",
    "    \n",
    "    # Prepare the results dictionary\n",
    "    results = {\n",
    "        'overall': reconstructions\n",
    "    }\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec73bf-a509-450e-ae49-53b486c8e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage (Replace with your dataset)\n",
    "# X = your_data_matrix_here\n",
    "results = main_single_analysis(X=X_train)\n",
    "# print(results)  # Inspect or visualize the reconstructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f06a26-ddc3-4f97-abe3-e935bd8adc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.image_processing import plot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f1c9e-6ae6-441c-bb25-ee44681821d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a08aa-a3ff-4f7e-8721-9004b3b73f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Normalize Data (no normalization for this run)\n",
    "def normalize_data(data, method='none'):\n",
    "    \"\"\"\n",
    "    Applies normalization to the data.\n",
    "    \"\"\"\n",
    "    if method == 'none':\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported normalization method for this test.\")\n",
    "\n",
    "# Step 2: Initialize and Fit NMF Model\n",
    "def fit_nmf_model(X, n_components=100):\n",
    "    \"\"\"\n",
    "    Fits an NMF model with the specified number of components.\n",
    "    \"\"\"\n",
    "    model = NMF(n_components=n_components, init='nndsvda', max_iter=500, random_state=42)\n",
    "    W = model.fit_transform(X)  # Transformed matrix (basis)\n",
    "    H = model.components_       # Coefficients matrix\n",
    "    return model, W, H\n",
    "\n",
    "# Step 3: Generate Partial Reconstructions\n",
    "def generate_partial_reconstructions(model, W, H, reconstruction_components):\n",
    "    \"\"\"\n",
    "    Generates partial reconstructions for a given NMF model and specified components.\n",
    "    \"\"\"\n",
    "    reconstructions = []\n",
    "    for n in reconstruction_components:\n",
    "        # Truncate components to the top `n`\n",
    "        H_partial = H[:n, :]\n",
    "        W_partial = W[:, :n]\n",
    "        # Reconstruct using the truncated components\n",
    "        X_reconstructed = np.dot(W_partial, H_partial)\n",
    "        reconstructions.append(X_reconstructed)\n",
    "    return reconstructions\n",
    "\n",
    "# Step 4: Average and Reshape Reconstructions\n",
    "def process_reconstructed_images(reconstructions, image_shape=(48, 48)):\n",
    "    \"\"\"\n",
    "    Averages reconstructions across all samples and reshapes them for visualization.\n",
    "    \n",
    "    Parameters:\n",
    "        reconstructions (list): List of reconstructed datasets (arrays).\n",
    "        image_shape (tuple): Target shape for each reconstructed image (default: 48x48).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of reshaped, averaged reconstructed images.\n",
    "    \"\"\"\n",
    "    averaged_images = []\n",
    "    for reconstruction in reconstructions:\n",
    "        # Average across samples\n",
    "        average_image = np.mean(reconstruction, axis=0)\n",
    "        # Reshape to the specified image shape\n",
    "        reshaped_image = average_image.reshape(image_shape)\n",
    "        averaged_images.append(reshaped_image)\n",
    "    return averaged_images\n",
    "\n",
    "# Main Analysis\n",
    "def main_single_analysis(X):\n",
    "    \"\"\"\n",
    "    Performs a single analysis with NMF (100 components) and specified partial reconstructions.\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    n_components = 100\n",
    "    reconstruction_components = [1, 10, 25, 50, 75, 100]\n",
    "    \n",
    "    # Step 1: Normalize the data\n",
    "    X_normalized = normalize_data(X, method='none')\n",
    "    \n",
    "    # Step 2: Fit NMF model\n",
    "    model, W, H = fit_nmf_model(X_normalized, n_components=n_components)\n",
    "    \n",
    "    # Step 3: Generate partial reconstructions\n",
    "    reconstructions = generate_partial_reconstructions(\n",
    "        model, W, H, reconstruction_components\n",
    "    )\n",
    "    \n",
    "    # Step 4: Average and reshape reconstructions\n",
    "    averaged_images = process_reconstructed_images(reconstructions, image_shape=(48, 48))\n",
    "    \n",
    "    # Prepare the results dictionary\n",
    "    results = {\n",
    "        'overall': averaged_images\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Example Usage (Replace with your dataset)\n",
    "# X = your_data_matrix_here\n",
    "# results = main_single_analysis(X)\n",
    "# print(results)  # Inspect or visualize the averaged and reshaped reconstructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee026ce-ae89-4f4a-b4a4-ba7f56ce0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = main_single_analysis(X_train)\n",
    "# print(results)  # Inspect or visualize the averaged and reshaped reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38572543-64c7-47ea-9f80-20b09eb831db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63540761-c68a-4ae8-aa90-0dabcbb5b272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb8805-a793-424f-93ae-28df4eac2d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d7217-94a2-4af7-877e-41652c0072ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced7335-8ac8-4e51-a081-158fc9db2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Normalize Data (no normalization for this run)\n",
    "def normalize_data(data, method='none'):\n",
    "    \"\"\"\n",
    "    Applies normalization to the data.\n",
    "    \"\"\"\n",
    "    if method == 'none':\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported normalization method for this test.\")\n",
    "\n",
    "# Step 2: Initialize and Fit PCA Model\n",
    "def fit_pca_model(X, n_components=100):\n",
    "    \"\"\"\n",
    "    Fits a PCA model with the specified number of components.\n",
    "    \"\"\"\n",
    "    model = PCA(n_components=n_components, svd_solver='full', random_state=42)\n",
    "    transformed_data = model.fit_transform(X)  # Transformed data (projections)\n",
    "    components = model.components_             # Principal components\n",
    "    mean = model.mean_                         # Mean used for centering\n",
    "    return model, transformed_data, components, mean\n",
    "\n",
    "# Step 3: Generate Partial Reconstructions\n",
    "def generate_partial_reconstructions_pca(model, transformed_data, components, mean, reconstruction_components):\n",
    "    \"\"\"\n",
    "    Generates partial reconstructions for a given PCA model and specified components.\n",
    "    \"\"\"\n",
    "    reconstructions = []\n",
    "    for n in reconstruction_components:\n",
    "        # Reconstruct using the top `n` components\n",
    "        components_partial = components[:n, :]\n",
    "        transformed_partial = transformed_data[:, :n]\n",
    "        X_reconstructed = np.dot(transformed_partial, components_partial) + mean\n",
    "        X_reconstructed = np.clip(X_reconstructed, 0, 255)  # Ensure valid pixel range\n",
    "        reconstructions.append(X_reconstructed)\n",
    "    return reconstructions\n",
    "\n",
    "# Step 4: Average and Reshape Reconstructions\n",
    "def process_reconstructed_images(reconstructions, image_shape=(48, 48)):\n",
    "    \"\"\"\n",
    "    Averages reconstructions across all samples and reshapes them for visualization.\n",
    "    \n",
    "    Parameters:\n",
    "        reconstructions (list): List of reconstructed datasets (arrays).\n",
    "        image_shape (tuple): Target shape for each reconstructed image (default: 48x48).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of reshaped, averaged reconstructed images.\n",
    "    \"\"\"\n",
    "    averaged_images = []\n",
    "    for reconstruction in reconstructions:\n",
    "        # Average across samples\n",
    "        average_image = np.mean(reconstruction, axis=0)\n",
    "        # Reshape to the specified image shape\n",
    "        reshaped_image = average_image.reshape(image_shape)\n",
    "        averaged_images.append(reshaped_image)\n",
    "    return averaged_images\n",
    "\n",
    "# Main Analysis\n",
    "def main_single_analysis_pca(X):\n",
    "    \"\"\"\n",
    "    Performs a single analysis with PCA (100 components) and specified partial reconstructions.\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    n_components = 100\n",
    "    reconstruction_components = [1, 10, 25, 50, 75, 100]\n",
    "    \n",
    "    # Step 1: Normalize the data\n",
    "    X_normalized = normalize_data(X, method='none')\n",
    "    \n",
    "    # Step 2: Fit PCA model\n",
    "    model, transformed_data, components, mean = fit_pca_model(X_normalized, n_components=n_components)\n",
    "    \n",
    "    # Step 3: Generate partial reconstructions\n",
    "    reconstructions = generate_partial_reconstructions_pca(\n",
    "        model, transformed_data, components, mean, reconstruction_components\n",
    "    )\n",
    "    \n",
    "    # Step 4: Average and reshape reconstructions\n",
    "    averaged_images = process_reconstructed_images(reconstructions, image_shape=(48, 48))\n",
    "    \n",
    "    # Prepare the results dictionary\n",
    "    results = {\n",
    "        'overall': averaged_images\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Visualization Function\n",
    "def plot_reconstructed_images(images, reconstruction_components):\n",
    "    \"\"\"\n",
    "    Plots reconstructed images.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    for i, img in enumerate(images):\n",
    "        plt.figure()\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f\"Reconstruction with {reconstruction_components[i]} components\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675c835-414f-42ef-8472-0a078c9858be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "results = main_single_analysis_pca(X_train)\n",
    "\n",
    "# Visualize results\n",
    "plot_reconstructed_images(results['overall'], [1, 10, 25, 50, 75, 100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888189e8-70df-48ba-9da1-2edadbf5df2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cfc74-3220-43fc-b231-a0c08a3da811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b75dc-9878-4da3-9ece-b97d881e0e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_identical_images(images):\n",
    "    \"\"\"\n",
    "    Checks if all images in a list are nearly identical using np.allclose.\n",
    "    \n",
    "    Parameters:\n",
    "        images (list): List of reconstructed images (2D arrays).\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if all images are nearly identical, False otherwise.\n",
    "        list: List of tuples indicating pairs of images that are identical.\n",
    "    \"\"\"\n",
    "    identical_pairs = []\n",
    "    for i in range(len(images)):\n",
    "        for j in range(i + 1, len(images)):\n",
    "            if np.allclose(images[i], images[j]):\n",
    "                identical_pairs.append((i, j))\n",
    "    \n",
    "    all_identical = len(identical_pairs) == len(images) * (len(images) - 1) // 2\n",
    "    return all_identical, identical_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165dddba-d6c6-448e-8a79-7d79169b1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if images are identical\n",
    "all_identical, identical_pairs = check_identical_images(results['overall'])\n",
    "\n",
    "if all_identical:\n",
    "    print(\"All images are nearly identical.\")\n",
    "else:\n",
    "    print(\"Not all images are identical.\")\n",
    "    print(f\"Identical image pairs: {identical_pairs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a0f97-18af-412f-a883-0e2229707ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['overall'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43298a65-c15e-4205-8f77-9a993b3280b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['overall'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03df439-95d6-4e56-a717-5411c9a90461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract two images and compute mean difference\n",
    "mean_diff = np.mean(results['overall'][0] - results['overall'][1])\n",
    "print(\"Mean difference:\", mean_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d47e80-39c9-4912-8971-2d3579a13f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract two images and compute mean difference\n",
    "mean_diff = np.mean(results['overall'][0] - results['overall'][5])\n",
    "print(\"Mean difference:\", mean_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e01f4-c800-4ea2-bcc2-27678ffaaddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88fc315-1b2a-4d12-807f-24f67f9b03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(image1, image2):\n",
    "    \"\"\"\n",
    "    Computes Mean Squared Error (MSE) between two images.\n",
    "    \n",
    "    Parameters:\n",
    "        image1 (ndarray): First image.\n",
    "        image2 (ndarray): Second image.\n",
    "    \n",
    "    Returns:\n",
    "        float: MSE value.\n",
    "    \"\"\"\n",
    "    return np.mean((image1 - image2) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26d1b7-c02c-4a01-9fc5-b9e2e880fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def compute_ssim(image1, image2):\n",
    "    \"\"\"\n",
    "    Computes Structural Similarity Index (SSIM) between two images.\n",
    "    \n",
    "    Parameters:\n",
    "        image1 (ndarray): First image.\n",
    "        image2 (ndarray): Second image.\n",
    "    \n",
    "    Returns:\n",
    "        float: SSIM value.\n",
    "    \"\"\"\n",
    "    return ssim(image1, image2, data_range=image1.max() - image1.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c321039-2592-4fb3-8242-b166b0634d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896dd89-0fe3-429a-b91e-3ad80cecec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def main_single_analysis_pca_extended(X_train, n_components, normalization_method, output_dir=None):\n",
    "    \"\"\"\n",
    "    Performs PCA with flexible components and normalization options,\n",
    "    allowing for storage and comparison of results.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (array-like): Input data (e.g., training images).\n",
    "        n_components (int): Number of PCA components to use.\n",
    "        normalization_method (str): Normalization method ('standard' or 'none').\n",
    "        output_dir (str): Directory to save results (optional).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains reconstructions, explained variance, and summary details.\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    reconstruction_components = [1, 10, 25, 50, 75, 100]\n",
    "    \n",
    "    # Step 1: Normalize the data\n",
    "    if normalization_method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "        X_normalized = scaler.fit_transform(X_train)\n",
    "    elif normalization_method == 'none':\n",
    "        X_normalized = X_train\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported normalization method: {normalization_method}\")\n",
    "    \n",
    "    # Step 2: Fit PCA model and retrieve variance\n",
    "    model, transformed_data, components, mean, explained_variance = fit_pca_model(\n",
    "        X_normalized, n_components=n_components\n",
    "    )\n",
    "    \n",
    "    # Step 3: Generate partial reconstructions\n",
    "    reconstructions = generate_partial_reconstructions_pca(\n",
    "        model, transformed_data, components, mean, reconstruction_components\n",
    "    )\n",
    "    \n",
    "    # Step 4: Average and reshape reconstructions\n",
    "    averaged_images = process_reconstructed_images(reconstructions, image_shape=(48, 48))\n",
    "    \n",
    "    # Store results\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    results = {\n",
    "        'n_components': n_components,\n",
    "        'normalization_method': normalization_method,\n",
    "        'reconstruction_components': reconstruction_components,\n",
    "        'overall': averaged_images,\n",
    "        'explained_variance': explained_variance,\n",
    "        'cumulative_variance': cumulative_variance\n",
    "    }\n",
    "    \n",
    "    # Print variance explained\n",
    "    print(f\"\\nResults for PCA with {n_components} components and {normalization_method} normalization:\")\n",
    "    for i, var in enumerate(cumulative_variance[:10], start=1):\n",
    "        print(f\"  Component {i}: Cumulative Variance = {var:.4f}\")\n",
    "    print(f\"  Total Variance Explained with {n_components} components: {cumulative_variance[-1]:.4f}\")\n",
    "    \n",
    "    # Save results to output directory\n",
    "    if output_dir:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = os.path.join(output_dir, f\"pca_results_{n_components}_{normalization_method}_{timestamp}.npz\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        np.savez(output_path, **results)\n",
    "        print(f\"Results saved to {output_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_multiple_pca_variations(X_train, variations, output_dir=None):\n",
    "    \"\"\"\n",
    "    Runs multiple PCA variations and stores results for comparison.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (array-like): Input data (e.g., training images).\n",
    "        variations (list of dict): List of parameter dictionaries for each PCA run.\n",
    "        output_dir (str): Directory to save results (optional).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of results from all runs.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for params in variations:\n",
    "        print(f\"\\nRunning PCA with parameters: {params}\")\n",
    "        result = main_single_analysis_pca_extended(\n",
    "            X_train,\n",
    "            n_components=params['n_components'],\n",
    "            normalization_method=params['normalization_method'],\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        all_results.append(result)\n",
    "    \n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3b23f-beb1-4768-8dab-f1a4abf61105",
   "metadata": {},
   "outputs": [],
   "source": [
    "variations = [\n",
    "    {'n_components': 100, 'normalization_method': 'standard'},\n",
    "    {'n_components': 100, 'normalization_method': 'none'},\n",
    "    {'n_components': 200, 'normalization_method': 'standard'},\n",
    "    {'n_components': 300, 'normalization_method': 'standard'}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610b122-a286-4c20-b120-a1f43a42ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"pca_results\"\n",
    "results = run_multiple_pca_variations(X_train, variations, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb4ae1a-49d5-4135-84e6-a6039450e60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca61fa-a740-44f7-aef0-4fbba1907388",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    print(f\"Number of components: {r['n_components']}\")\n",
    "    print(f\"Normalization: {r['normalization_method']}\")\n",
    "    plot_reconstructed_images(r['overall'], r['reconstruction_components'])\n",
    "    print('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63505782-cf64-4b95-9ea6-7fd5f3fe71e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e47fc-e68e-4540-8b2f-d9739555fcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9d093-e0fa-4e28-b52c-7b43d8dc1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# composite_face_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b10db-89aa-4718-a3d6-a986466397df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(composite_face_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c7d5f5-b96f-42f9-b898-89caee7f79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# composite_face_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b656f-b3a8-4c0e-9d8a-fd109ec054db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d0f79-0e20-4891-a0c4-2810c9b8b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "\n",
    "def compute_mse(image1, image2):\n",
    "    \"\"\"\n",
    "    Computes Mean Squared Error (MSE) between two images.\n",
    "    \"\"\"\n",
    "    return np.mean((image1 - image2) ** 2)\n",
    "\n",
    "def compute_ssim(image1, image2):\n",
    "    \"\"\"\n",
    "    Computes Structural Similarity Index (SSIM) between two images.\n",
    "    \"\"\"\n",
    "    return ssim(image1, image2, data_range=image1.max() - image1.min())\n",
    "\n",
    "def main_single_analysis_pca_extended_with_metrics(X_train, n_components, normalization_method, output_dir=None):\n",
    "    \"\"\"\n",
    "    Performs PCA with flexible components and normalization options, and evaluates MSE and SSIM\n",
    "    for each reconstruction relative to the original data.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (array-like): Input data (e.g., training images).\n",
    "        n_components (int): Number of PCA components to use.\n",
    "        normalization_method (str): Normalization method ('standard', 'none', 'minmax').\n",
    "        output_dir (str): Directory to save results (optional).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains reconstructions, explained variance, metrics, and summary details.\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    reconstruction_components = [1, 10, 25, 50, 75, 100]\n",
    "    \n",
    "    # Step 1: Normalize the data\n",
    "    if normalization_method == 'standard':\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        X_normalized = scaler.fit_transform(X_train)\n",
    "    elif normalization_method == 'none':\n",
    "        X_normalized = X_train\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported normalization method: {normalization_method}\")\n",
    "    \n",
    "    # Step 2: Fit PCA model and retrieve variance\n",
    "    from sklearn.decomposition import PCA\n",
    "    model = PCA(n_components=n_components, svd_solver='full', random_state=42)\n",
    "    transformed_data = model.fit_transform(X_normalized)  # Transformed data (projections)\n",
    "    components = model.components_             # Principal components\n",
    "    mean = model.mean_                         # Mean used for centering\n",
    "    explained_variance = model.explained_variance_ratio_  # Variance explained by each component\n",
    "    \n",
    "    # Generate partial reconstructions\n",
    "    reconstructions = []\n",
    "    for n in reconstruction_components:\n",
    "        # Reconstruct using the top `n` components\n",
    "        components_partial = components[:n, :]\n",
    "        transformed_partial = transformed_data[:, :n]\n",
    "        X_reconstructed = np.dot(transformed_partial, components_partial) + mean\n",
    "        X_reconstructed = np.clip(X_reconstructed, 0, 255)  # Ensure valid pixel range\n",
    "        reconstructions.append(X_reconstructed)\n",
    "    \n",
    "    # Evaluate metrics for each reconstruction\n",
    "    mse_scores = []\n",
    "    ssim_scores = []\n",
    "    for reconstruction in reconstructions:\n",
    "        mse = compute_mse(X_train, reconstruction)\n",
    "        mse_scores.append(mse)\n",
    "        \n",
    "        ssim_mean = np.mean([\n",
    "            compute_ssim(X_train[i].reshape(48, 48), reconstruction[i].reshape(48, 48))\n",
    "            for i in range(len(X_train))\n",
    "        ])\n",
    "        ssim_scores.append(ssim_mean)\n",
    "    \n",
    "    # Average and reshape reconstructions\n",
    "    averaged_images = []\n",
    "    for reconstruction in reconstructions:\n",
    "        average_image = np.mean(reconstruction, axis=0)\n",
    "        averaged_images.append(average_image.reshape(48, 48))\n",
    "    \n",
    "    # Store results\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    results = {\n",
    "        'n_components': n_components,\n",
    "        'normalization_method': normalization_method,\n",
    "        'reconstruction_components': reconstruction_components,\n",
    "        'overall': averaged_images,\n",
    "        'mse_scores': mse_scores,\n",
    "        'ssim_scores': ssim_scores,\n",
    "        'explained_variance': explained_variance.tolist(),\n",
    "        'cumulative_variance': cumulative_variance.tolist()\n",
    "    }\n",
    "    \n",
    "    # Print variance explained\n",
    "    print(f\"\\nResults for PCA with {n_components} components and {normalization_method} normalization:\")\n",
    "    for i, var in enumerate(cumulative_variance[:10], start=1):\n",
    "        print(f\"  Component {i}: Cumulative Variance = {var:.4f}\")\n",
    "    print(f\"  Total Variance Explained with {n_components} components: {cumulative_variance[-1]:.4f}\")\n",
    "    \n",
    "    # Print MSE and SSIM metrics\n",
    "    print(\"\\nMetrics by Reconstruction Components:\")\n",
    "    for i, comp in enumerate(reconstruction_components):\n",
    "        print(f\"  Components {comp}: MSE = {mse_scores[i]:.4f}, SSIM = {ssim_scores[i]:.4f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb84b50-c276-4f9c-94f2-d89d1ec9feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_analysis_results(output_dir, analysis_name, images, settings, metrics, matrix_plot):\n",
    "    \"\"\"\n",
    "    Saves results of an analysis into a structured directory. Skips existing directories.\n",
    "    \n",
    "    Parameters:\n",
    "        output_dir (str): Base output directory.\n",
    "        analysis_name (str): Name of the analysis (e.g., \"pca_100_standard\").\n",
    "        images (dict): Dictionary of averaged reconstructed images by category.\n",
    "        settings (dict): Dictionary of analysis settings.\n",
    "        metrics (dict): Dictionary of evaluation metrics (mse, ssim).\n",
    "        matrix_plot (np.ndarray): Grid of final reconstructed images for plotting.\n",
    "    \"\"\"\n",
    "    # Create analysis directory\n",
    "    analysis_dir = os.path.join(output_dir, analysis_name)\n",
    "    \n",
    "    if os.path.exists(analysis_dir):\n",
    "        print(f\"[SKIP] Directory already exists: {analysis_dir}\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(analysis_dir, exist_ok=True)\n",
    "    \n",
    "    # Save reconstructed images (compressed)\n",
    "    images_path = os.path.join(analysis_dir, \"images.npz\")\n",
    "    np.savez_compressed(images_path, **images)\n",
    "    \n",
    "    # Save settings as JSON\n",
    "    settings_path = os.path.join(analysis_dir, \"settings.json\")\n",
    "    with open(settings_path, 'w') as f:\n",
    "        json.dump(settings, f, indent=4)\n",
    "    \n",
    "    # Save metrics as CSV\n",
    "    metrics_path = os.path.join(analysis_dir, \"metrics.csv\")\n",
    "    with open(metrics_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Category\", \"Components\", \"MSE\", \"SSIM\"])  # Header row\n",
    "        for category, metric_data in metrics.items():\n",
    "            for i, components in enumerate(settings['reconstruction_components']):\n",
    "                writer.writerow([category, components, metric_data['mse_scores'][i], metric_data['ssim_scores'][i]])\n",
    "    \n",
    "    # Save matrix plot as PNG\n",
    "    matrix_plot_path = os.path.join(analysis_dir, \"matrix_plot.png\")\n",
    "    plt.imsave(matrix_plot_path, matrix_plot, cmap='gray')\n",
    "\n",
    "    print(f\"[SAVED] Results for {analysis_name} saved in: {analysis_dir}\")\n",
    "\n",
    "\n",
    "def generate_matrix_plot(images, reconstruction_components, categories, image_shape=(48, 48)):\n",
    "    \"\"\"\n",
    "    Generates a matrix plot of averaged reconstructed images.\n",
    "    \n",
    "    Parameters:\n",
    "        images (dict): Dictionary of reconstructed images by category.\n",
    "        reconstruction_components (list): List of reconstruction levels.\n",
    "        categories (list): List of categories (including 'Overall').\n",
    "        image_shape (tuple): Shape of the images (default: (48, 48)).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A matrix plot as a NumPy array.\n",
    "    \"\"\"\n",
    "    num_components = len(reconstruction_components)\n",
    "    num_categories = len(categories)\n",
    "    \n",
    "    # Create a blank canvas\n",
    "    grid_height = num_components * image_shape[0]\n",
    "    grid_width = num_categories * image_shape[1]\n",
    "    matrix_plot = np.zeros((grid_height, grid_width))\n",
    "    \n",
    "    # Fill the canvas with reconstructed images\n",
    "    for i, comp in enumerate(reconstruction_components):\n",
    "        for j, category in enumerate(categories):\n",
    "            img = images[category][i]  # Averaged image for the component/category\n",
    "            row_start = i * image_shape[0]\n",
    "            col_start = j * image_shape[1]\n",
    "            matrix_plot[row_start:row_start + image_shape[0], col_start:col_start + image_shape[1]] = img\n",
    "    \n",
    "    return matrix_plot\n",
    "\n",
    "\n",
    "def run_pca_by_category_with_saving(X_train, y_train, n_components, normalization_method, output_dir):\n",
    "    \"\"\"\n",
    "    Runs PCA on the full dataset ('Overall') and each category, saving results for each analysis.\n",
    "    Skips existing directories and logs skipped analyses.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (array-like): Input data.\n",
    "        y_train (array-like): Labels corresponding to X_train.\n",
    "        n_components (int): Number of PCA components to use.\n",
    "        normalization_method (str): Normalization method ('standard', 'none', 'minmax').\n",
    "        output_dir (str): Base output directory for saving results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    reconstruction_components = [1, 10, 25, 50, 75, 100]\n",
    "    \n",
    "    # Automatically determine unique categories\n",
    "    categories = np.unique(y_train)\n",
    "    categories_with_overall = ['Overall'] + list(categories)\n",
    "    \n",
    "    # Run PCA for the overall dataset\n",
    "    print(\"\\nRunning PCA for Overall Dataset\")\n",
    "    overall_result = main_single_analysis_pca_extended_with_metrics(\n",
    "        X_train, n_components, normalization_method, output_dir=None\n",
    "    )\n",
    "    results['Overall'] = overall_result\n",
    "    \n",
    "    # Run PCA for each category\n",
    "    for category in categories:\n",
    "        print(f\"\\nRunning PCA for Category: {category}\")\n",
    "        # Mask data for the current category\n",
    "        X_category = X_train[y_train == category]\n",
    "        \n",
    "        category_result = main_single_analysis_pca_extended_with_metrics(\n",
    "            X_category, n_components, normalization_method, output_dir=None\n",
    "        )\n",
    "        results[category] = category_result\n",
    "    \n",
    "    # Save results\n",
    "    analysis_name = f\"pca_{n_components}_{normalization_method}\"\n",
    "    images = {key: value['overall'] for key, value in results.items()}\n",
    "    metrics = {key: {'mse_scores': value['mse_scores'], 'ssim_scores': value['ssim_scores']} for key, value in results.items()}\n",
    "    settings = {\n",
    "        'n_components': n_components,\n",
    "        'normalization_method': normalization_method,\n",
    "        'categories': categories_with_overall,\n",
    "        'reconstruction_components': reconstruction_components\n",
    "    }\n",
    "    matrix_plot = generate_matrix_plot(images, reconstruction_components, categories_with_overall)\n",
    "    save_analysis_results(output_dir, analysis_name, images, settings, metrics, matrix_plot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c051e-da46-479e-a845-543b7ca11bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da416d6e-dff2-41ea-bba0-d57b3f75ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# results_dir = \"unsupervised_models\"\n",
    "# n_components = 100\n",
    "# normalization_method = \"standard\"\n",
    "# categories = np.unique(y_train)\n",
    "\n",
    "# run_pca_by_category_with_saving(X_train, y_train, categories, n_components, normalization_method, results_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82687714-dcc6-4deb-8de4-6bc9f908094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "results_dir = \"unsupervised_models\"\n",
    "n_components = 100\n",
    "normalization_method = \"standard\"\n",
    "\n",
    "run_pca_by_category_with_saving(X_train, y_train, n_components, normalization_method, results_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2757d1-c679-45c5-b5fb-a314e30a460a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd1f0ae-daf1-45e5-a02c-aa5a3ab8c6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40914f8-b3a2-4ac1-b077-af797ab06b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38309c9-91fd-462c-bca8-903f4fe1ae20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f4345-2576-43c1-94c0-cc312e828640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
